\section{Экспериментальное исследование}

\subsection{Эксперименты на корпусе IBM Evidence Search}

Для первых экспериментов для извлечения структуры аргументации был выбран корпус IBM Evidence Search из работы \cite{shnarch2018will}. В рамках данной текстовой коллекции необходимо найти связь между темой и предпосылкой. Примеры:

\begin{verbatim}
    Тема: We should fight illegal immigration
    Предпосылка: A paper in the peer reviewed Tax Lawyer journal from the American 
    Bar Association asserts that illegal immigrants contribute more in taxes 
    than they cost in social services
    Тип отношения: релевантно
    
    Тема: We should abandon coal mining
    Предпосылка: In particular, Daintree was the first Government geologist for
    North Queensland discovering gold fields and coal seams for future exploitation.
    Тип отношения: нерелевантно
\end{verbatim}

В данном корпусе представлено 5700 пар тем и предпосылок по 118 темам, 35 из которых отведены в качестве тестовой выборки. Примеров положительного класса, т.е. пар релевантных темы и посылки и в тестовой и в обучающей части около 40\%. Дополнительно тестовая часть этого корпуса была переведена на русский язык для замеров эффективности межязыкового переноса.

Также рассматривается русскоязычный корпуса TERRa. В данном корпусе стоит задача логического вывода: необходимо по паре текста и гипотезы понять, можно ли вывести гипотезу из текста. Данная постановка похожа на постановку определения релевантности, поэтому было решено исследовать влияние обучения на корпусе TERRa на решение задачи определения структуры аргументации. Пример данных из корпуса TERRa:
\begin{verbatim}
    Текст: Женщину доставили в больницу, за ее жизнь сейчас борются врачи.
    Гипотеза: Женщину спасают врачи.
    Тип отношения: гипотезу можно вывести из текста
    
    Текст: Представитель внешнеполитического ведомства отметил, что злоумышленника
    задержали местные власти.
    Гипотеза: Злоумышленник напуган.
    Тип отношения: гипотеза не следует из текста.
\end{verbatim}

\begin{table}[h!]
\centering
\begin{tabular}{|| c | c | c |} 
 \hline
 Модель & Макро-Точность & Микро-Точность \\ [0.5ex] 
 \hline
 $BERT_{en}$ & \textbf{82.4} & \textbf{81.1} \\ 
 $BERT_{mult}$ & 81.6 & 80.5 \\
 $IBM BiLSTM$ & - & 72 \\
 $IBM BiLSTM_{ext}$ & - & 76 \\ 
 \hline
 $BERT_{mult}$ & 77.5 & 76.4 \\
 $BERT_{terraZS}$ & 66.7 & 60.3 \\
 $BERT_{comb}$ & 75.9 & 73.9 \\
 \hline
\end{tabular}
\caption{Результаты на тестовом корпусе IBM Evidence Search. Горизонтальной линией отделены результаты на англоязычной и русскоязычной версиях.}
\label{table:1}
\end{table}

В качестве метрики в исходной работе используется микро-точность по темам. В качестве моделей авторы \cite{shnarch2018will} предлагали двунаправленную LSTM над векторизацией слов GloVE. Данная модель обозначена как $IBM BiLSTM$. Авторы также исследовали влияние внешних данных, собранных автоматически на качество работы модели. Модель, использующая внешние данные называется $IBM BiLSTM_{ext}$. Автоматически собранный корпус не выложен в открытый доступ, поэтому исследовать его влияние на предложенные в данной дипломной работе модели не представляется возможным.

В качестве предложенных моделей для обучения на английском корпусе было решено взять архитектуру $BERT_{base}$, т.е. модель, одновременно обрабатывающую обе компоненты аргументации. В таблице 1 моделью $BERT_{en}$ называется модель на архитектуре $BERT_{base}$, дообученная с весов англоязычной версии $BERT$. Модели $BERT_{mult}$, $BERT_{terraZS}$, $BERT_{comb}$ дообучены с весов мультиязычной версии оригинальной модели $BERT$.

Как видно из таблицы использование англоязычной модели $BERT_{en}$ дает заметно лучшее качество, чем модели, основанные на архитектуре $BiLSTM$, даже с использованием внешних данных. На один пункт точности уменьшается качество мультиязычной модели $BERT_{mult}$ по сравнению с англоязычной, но качество все еще заметно выское.

Во второй половине таблицы отображены результаты работы модели $BERT_{mult}$ на русскоязычной версии корпуса. Происходит заметное падение качества на 5 пунктов, однако модель все еще извлекает аргументацию на уровне, сопоставимом с наилучшим решением.

Модель $BERT_{terraZS}$ была обучена на корпусе TERRa и применена в сценарии Zero Shot (без дообучения) на корпусе IBM Evidence Search. Модель показывает ненулевое качество. Дальнейшей попыткой применить корпус логического вывода TERRa является модель $BERT_{comb}$. Было решено объединить корпуса TERRa и IBM Evidence Search, чтобы понять как влияет корпус TERRa на задачу извлечения структуры аргументации. Полученная модель оказывается хуже аналогичной модели $BERT_{mult}$, обученной исключительно на англоязычных данных.

\subsection{Эксперименты на корпусах ArgsEN и EviEN}
В новой работе \cite{toledo2020multilingual} предоставлено 2 корпуса, посвященных аргументации: ArgsEN и EviEN. В корпусе ArgsEN на 30000 примеров размечена полемическая позиция между темами и утверждениями:

\begin{verbatim}
    Тема: We should abandon marriage
    Утверждение: \"marriage\" isn't keeping up with the times.  abandon the old
    thinking and bring something that incorporates all unions - not just those with a
    man and woman.
    Полемическая позиция: поддержка
    
    Тема: We should prohibit flag burning
    Утверждение: a flag is only really a peace of cloth and doesn't actually hurt
    anybody.
    Полемическая позиция: противоречие (атака)
\end{verbatim}

В корпусе EviEN похожим образом размечены отношения между темами и предпосылками (35000 пар), дополнительно размечена и релевантность. Таким образом корпус EviEN является единственным корпусом, на котором можно решать одновременно задачи извлечения струкутуры и определения полемической позиции. Примеры:

\begin{verbatim}
    Тема: We should ban the sale of violent video games to minors
    Предпосылка: Justice Thomas, in his dissent, considered that historically, 
    the Founding Fathers "believed parents to have complete authority over 
    their minor children and expected parents to direct the 
    development of those children
    Структура: релевантно
    Полемическая позиция: противоречие
    
    Тема: We should ban the sale of violent video games to minors
    Предпосылка: while owning guns is a legal right in most countries, 
    the illegal trade in guns continues to fuel conflict
    Структура: нерелевантно
    Полемическая позиция: нейтральная
\end{verbatim}

Также дополнительно рассматриваится и корпус для логического вывода SNLI. Данный корпус состоит из 570000 пар текстов и гипотез, необходимо определить поддерживается, опровергается гипотеза текстом или из текста нельзя сделать выводы о гипотезе. Примеры:

\begin{verbatim}
    Тема: A person on a horse jumps over a broken down airplane.
    Гипотеза: A person is outdoors, on a horse.
    Отношение: Подтверждение
\end{verbatim}

На корпусе ArgsEN проводились замеры качества определения полемической позиции аргументации, отображенные в таблице 2.


\begin{table}[h!]
\centering
\begin{tabular}{|| c | c |} 
 \hline
 Модель & Макро-F1 \\ [0.5ex] 
 \hline
 $IBM BERT_{en}$ & 89.3 \\
 $IBM BERT_{17L}$ & \textit{91.5} \\
 $BERT_{basic}$ & 90.1 \\
 $BERT_{vectorized}$ & 75.2 \\
 $BERT_{SE}$ & \textbf{90.3} \\
 $BERT_{snliZS}$ & 55.0 \\
 $BERT_{snliFT}$ & 87.9 \\
 \hline
\end{tabular}
\caption{Результаты на тестовом корпусе ArgsEN в задаче определения полемической позиции утверждения относительно темы. Метрика - макроусредненная F1 по темам.}
\label{table:1}
\end{table}

Авторы оригинальной работы представляют результаты моделей $IBM BERT_{en}$ и $IBM BERT_{17L}$. Архитектуры моделей не описываются, известно, что первая модель училась на английском корпусе, а вторая модель училась на неопубликованном корпусе из 17 языков. Вторая модель показывает наилучшие результаты и включена в сравнение для предоставление как лучшая обученная модель, однако предоставленные в данной дипломной работе модели справедливо сравнивать лишь с первой моделью, т.к. они учились исключительно на английском языке. 

Модели $BERT_{basic}$, $BERT_{vectorized}$, $BERT_{SE}$ начинали обучение с англоязычных весов модели $BERT$ и представляют собой архитектуры, описанные в 4 главе. Модель $BERT_{vectorized}$ из-за отсутствия перекрестного механизма внимания, вызванного независимой обработкой компонент аргументации показывает низкий результат. Значительно лучше ведет себя модель $BERT_{basic}$, способная смоделировать сложные завивимости между темой и утверждением, так как они подаются в модель одновременно. Модель со слоем интерпретации $BERT_{SE}$ показывает результат немного превосходящий базовую модель.

Дополнительно использовалась модель обученная на корпусе SNLI. В оригинальном корпусе SNLI присутствует 3 отношения: поддержка, атака и нейтральное отношение. Т.к. в данном корпусе нет нерелевантных пар темы и утверждения, из предсказаний модели выбирается наиболее вероятный класс из поддержки или атаки. В сценарии без дообучения (Zero Shot) модель $BERT_{snliZS}$ показывает низкий результат в 55 пунктов точности на задаче бинарной классификации. Дообученная модель $BERT_{snliFT}$ показывает также показывает более низкий результат, чем модель $BERT_{basic}$. Данный результат совпадает со схожим экспериментом на корпусах IBM Evidence Search и TERRa.

На корпусе EviEN тоже присутствует возможность выделять полемическую позицию, однако авторы работы \cite{toledo2020multilingual} не предоставляют для сравнения никаких модельных результатов. Модели, аналогичные моделям для корпуса ArgsEN представлены в таблице 3.

\begin{table}[h!]
\centering
\begin{tabular}{|| c | c |} 
 \hline
 Модель & Макро-F1 \\ [0.5ex] 
 \hline
 $BERT_{basic}$ & 64.8 \\
 $BERT_{vectorized}$ & 51.8 \\
 $BERT_{SE}$ & 67.6 \\
 \hline
\end{tabular}
\caption{Результаты на тестовом корпусе EviEN в задаче определения структуры и полемической позиции предпосылки относительно темы. Метрика - макроусредненная F1 по темам.}
\label{table:1}
\end{table}



Как отмечалось ранее, на корпусе EviEN поставлена вторая задача определения релевантности предпосылки по отношению к теме. Авторы корпуса не предоставляют своих модельных метрик. Метрики моделей из главы 4 предоставлены в таблице 4:

\begin{table}[h!]
\centering
\begin{tabular}{|| c | c |} 
 \hline
 Модель & Макро-F1 \\ [0.5ex] 
 \hline
 $BERT_{basic}$ & 68.5 \\
 $BERT_{vectorized}$ & 64.3 \\
 $BERT_{SE}$ & 72.8 \\
 \hline
\end{tabular}
\caption{Результаты на тестовом корпусе EviEN в задаче определения структуры аргументации (релевантности). Метрика - макроусредненная F1 по темам.}
\label{table:1}
\end{table}

Результаты на корпусе ArgsEN превосходят модели, предложенные в работе \cite{toledo2020multilingual} по метрикам. На всех корпусах лучше всего себя показывает модель $BERT_{SE}$ со слоем интерпретации, а хуже всего модель $BERT_{vectorized}$.


\subsection{Обнаружение пропаганды}
Корпус пропаганды состоял из ручной разметки новостных статей за 2019 год. Всего было обработано 438 статей, содержащих 21230 предложений, из которых 7485 содержали пропаганду.

В качестве первого трека были даны новостные статьи и было необходимо выделить участки текста, содержащие пропаганду.

\begin{table}[h!]
\centering
\begin{tabular}{|| c | c | c | c |} 
 \hline
 Модель & F1 & Precision & Recall \\ [0.5ex] 
 \hline
 $BiLSTM_{glove+charlstm}$ & 34.9 & 30.4 & 41.1 \\
 $BiLSTM_{ELMO}$ & 34.5 & 32.7 & 36.7 \\
 $BERT_{linear}$ & 40.8 & 35.7 & 47.7 \\
 $BERT_{crf}$ & 36.2 & 30.1 & 45.4 \\
 $BERT_{lstm+crf}$ & 41.4 & 36.3 & 48.2 \\
 $LaserTagger$ & 42.0 & 38.4 & 46.4 \\
 $LaserTagger{tf}$ & 45.1 & 42.3 & 48.2 \\
 $LaserTagger{tf+ls}$ & 46.1 & 40.6 & 53.3 \\
 \hline
 $LasterTagger_{TF+LS}$ & 44.6 & 55.6 & 37.3 \\
 $Hitachi$ & 51.5 & 56.5 & 47.3 \\
 \hline
\end{tabular}
\caption{Результаты на задаче выделения пропаганды. В качестве метрики используется посимвольная F1-мера. Чертой отделены замеры на валидационной выборке и тестовой.}
\label{table:1}
\end{table}

В качестве базовых моделей были предложены архитектуры на основе рекуррентной нейросети BiLSTM. Модель $BiLSTM_{glove+charlstm}$ использует для векторизации эмбеддинги GloVe и буквенный рекуррентный слой BiLSTM. Данный подход позволяет обрабатывать слова, которые не встречались в исходном словаре GloVe. Второй вариацией данной модели является векторизация на основе буквенной нейросети ElMO \cite{Peters:2018}. Из таблицы 5 видно, что два этих подхожа не имеют большой качественной разницы.

Далее ведется работа с англоязычной версией нейросетевой модели $BERT$. Модель, в которой слова векторизуются $BERT$ и подаются в линейный слой называется $BERT_{linear}$, что дает большой прирост качества относительно базовых моделей. Минусом данной модели является независимое предсказание последовательности меток. Для решения данной проблемы использовалась нейросеть $BERT_{crf}$, в которой слой Conditional Random Field одновременно оценивает всю последовательность меток, однако данный метод не дает улчшения, а наоборот ухудшает результат.

В качестве продолжения идеи научить модель связанно предсказывать метки была принята архитекутра $LaserTagger$. Данная модель работает аналогично моделям машинного перевода - исходная последовательность проходит векторизацию моделью $BERT$, после чего каждому слову в соответстиве ставится метка. В качестве модификаций данной модели были предложены методики Teacher Forcing и Label Smoothing, комбинация которых и вошла в финальную модель с качеством 44.6 посимвольной F1-меры.


\begin{table}[h!]
\centering
\begin{tabular}{|| c | c |} 
 \hline
 Модель & Микро-F1 \\ [0.5ex] 
 \hline
 $BERT_{CLS}$ & 57.3 \\
 $R-BERT$ & 59.2 \\
 $R-BERT_{ft}$ & 58.4 \\
 $R-BERT_{w}$ & 59.0 \\
 $R-BERT_{w+ft}$ & 59.0 \\
 $Ensemble$ & 60.6 \\
 \hline
 $Ensemble$ & 58.2 \\
 $ApplicaAI$ & 62.07 \\
 \hline
\end{tabular}
\caption{Результаты на задаче классификации пропаганды. В качестве метрики используется микро-F1 по классам пропаганды. Чертой отделены замеры на валидационной выборке и тестовой.}
\label{table:1}
\end{table}

Для классификации участков пропаганды была выбрана модель $R-BERT$. Для работы данной модели необходимо обрамить участки с пропагандой спецсимволами, чтобы указать модели, какой участок текста является наиболее важным. В базовом варианте для классификации бралось усреднение контекстных векторных представлений выделенного сегмента. В модели $R-BERT_{w}$ модель выучивала веса для участков текста и брала взвешенную сумму их векторных представлений для классификации. В модели $R-BERT_{ft}$ исходная модель $BERT$ дообучалась на доменном корпусе новостей, что дало небольшой прирост качества. В финальной модели использовался ансамбль всех моделей, собранный в голосующий классификатор.


Стоит отметить, что в корпусе прослеживается сильное смещение тем между тестовой выборкой и тренировочной, валидационной. Из-за этого метрики всех участников на валидационной выборке на 10-15\% выше, чем итоговые результаты на тесте. Команды Hitachi и ApplicaAI применили модели, схожие с $BERT_{linear}$, однако использовали внешние данные и методики доразметки данных моделью для радикального увеличения тренировочной выборки. По результатам\cite{dimov-etal-2020-nopropaganda} участия было занято 7 место из 36 в первом треке и 6 из 31 во втором треке.
